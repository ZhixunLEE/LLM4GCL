default:
  lm: 'LLaMA' # ['RoBERTa', 'LLaMA']
  seed: [0, 1, 2, 3, 4]
  cache: '/YOUR_PATH/hidden_states/'
  lr: 5e-5
  weight_decay: 5e-4
  epochs: 500
  valid_epoch: 10
  patience: 20
  batch_size: 256
  cache_batch_size: 30
  max_length: 256

  gnn: 'GCN'
  layer_num: 1
  hidden_dim: 64
  dropout: 0.5
  num_heads: 4 # only available for GATConv
  aggr: 'mean' # ['mean', 'max', 'lstm'], only available for SAGEConv

  engine_layer_select: [0, 6, 12, 18, 24, -1]
  engine_T: 0.1
  engine_r: 32
search_space:
  lr: [1e-5, 1e-4]
  gnn: ['GCN', 'SAGE']
  hidden_dim: [64, 128, 256]
  engine_r: [1, 32]
  # engine_layer_select: [0,5,10,15,20,-1]
